# YAML Extension of Base configuration for Kafka Controllers to avoid Duplication
x-kafka-controller-base: &kafka-controller-base
  image: apache/kafka:3.8.0
  restart: always
  environment: &kafka-controller-env
    KAFKA_PROCESS_ROLES: controller
    KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
    KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT
    KAFKA_LOG_DIRS: ${KAFKA_LOG_DIRS}
    KAFKA_CONTROLLER_QUORUM_VOTERS: 0@kafka-controller-0:9093,1@kafka-controller-1:9093,2@kafka-controller-2:9093

# YAML Extension of Base configuration for Kafka Brokers to avoid Duplication
x-kafka-broker-base: &kafka-broker-base
    image: apache/kafka:3.8.0
    restart: always
    environment: &kafka-broker-env
      KAFKA_PROCESS_ROLES: broker
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL_HOST:SSL
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_SSL_KEYSTORE_TYPE: ${KAFKA_SSL_KEYSTORE_TYPE}
      KAFKA_SSL_TRUSTSTORE_TYPE: ${KAFKA_SSL_TRUSTSTORE_TYPE}
      KAFKA_CONTROLLER_QUORUM_VOTERS: 0@kafka-controller-0:9093,1@kafka-controller-1:9093,2@kafka-controller-2:9093
      KAFKA_SSL_KEYSTORE_LOCATION: ${KAFKA_SSL_KEYSTORE_LOCATION}
      KAFKA_SSL_KEYSTORE_PASSWORD: ${KAFKA_SSL_KEYSTORE_PASSWORD}
      KAFKA_SSL_TRUSTSTORE_LOCATION: ${KAFKA_SSL_TRUSTSTORE_LOCATION}
      KAFKA_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: ${KAFKA_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM}
      KAFKA_LOG_DIRS: ${KAFKA_LOG_DIRS}
      KAFKA_SSL_CLIENT_AUTH: ${KAFKA_SSL_CLIENT_AUTH}
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: ${KAFKA_AUTO_CREATE_TOPICS_ENABLE}
      KAFKA_DEFAULT_REPLICATION_FACTOR: ${KAFKA_DEFAULT_REPLICATION_FACTOR}
      KAFKA_LOG_RETENTION_MS: ${KAFKA_LOG_RETENTION_MS}
      KAFKA_LOG_RETENTION_BYTES: ${KAFKA_LOG_RETENTION_BYTES}

services:
  # Controllers
  kafka-controller-0:
    <<: *kafka-controller-base
    environment:
      <<: *kafka-controller-env
      KAFKA_NODE_ID: 0
      KAFKA_LISTENERS: CONTROLLER://kafka-controller-0:9093
    volumes:
      - kafka_controller_data_0:/var/lib/kafka/data

  kafka-controller-1:
    <<: *kafka-controller-base
    environment:
      <<: *kafka-controller-env
      KAFKA_NODE_ID: 1
      KAFKA_LISTENERS: CONTROLLER://kafka-controller-1:9093
    volumes:
      - kafka_controller_data_1:/var/lib/kafka/data

  kafka-controller-2:
    <<: *kafka-controller-base
    environment:
      <<: *kafka-controller-env
      KAFKA_NODE_ID: 2
      KAFKA_LISTENERS: CONTROLLER://kafka-controller-2:9093 
    volumes:
      - kafka_controller_data_2:/var/lib/kafka/data

  # Brokers
  kafka-broker-0:
    <<: *kafka-broker-base
    ports:
      - "9094:9094"
    environment:
      <<: *kafka-broker-env
      KAFKA_NODE_ID: 4
      KAFKA_LISTENERS: PLAINTEXT://:9092,SSL_HOST://:9094
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker-0:9092,SSL_HOST://kafka-broker-0:9094
    volumes:
      - kafka_data_0:/var/lib/kafka/data
      - ./Certificates:/etc/kafka/secrets
    # logging:
    #   driver: gelf
    #   options:
    #     gelf-address: ${GELF_ADDRESS}
    #     tag: "kafka-broker-0"
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "1.0"
    #       memory: "1G"
    depends_on:
      # - logstash
      - kafka-controller-0
      - kafka-controller-1
      - kafka-controller-2

  kafka-broker-1:
    <<: *kafka-broker-base
    ports:
      - "9095:9095"
    environment:
      <<: *kafka-broker-env
      KAFKA_NODE_ID: 5
      KAFKA_LISTENERS: PLAINTEXT://:9092,SSL_HOST://:9095
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker-1:9092,SSL_HOST://kafka-broker-1:9095
    volumes:
      - kafka_data_1:/var/lib/kafka/data
      - ./Certificates:/etc/kafka/secrets
    # logging:
    #   driver: gelf
    #   options:
    #     gelf-address: ${GELF_ADDRESS}
    #     tag: "kafka-broker-1"
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "1.0"
    #       memory: "1G"
    depends_on:
      # - logstash
      - kafka-controller-0
      - kafka-controller-1
      - kafka-controller-2

  kafka-broker-2:
    <<: *kafka-broker-base
    ports:
      - "9096:9096"
    environment:
      <<: *kafka-broker-env
      KAFKA_NODE_ID: 6
      KAFKA_LISTENERS: PLAINTEXT://:9092,SSL_HOST://:9096
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker-2:9092,SSL_HOST://kafka-broker-2:9096
    volumes:
      - kafka_data_2:/var/lib/kafka/data
      - ./Certificates:/etc/kafka/secrets
    # logging:
    #   driver: gelf
    #   options:
    #     gelf-address: ${GELF_ADDRESS}
    #     tag: "kafka-broker-2"
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "1.0"
    #       memory: "1G"
    depends_on:
      # - logstash
      - kafka-controller-0
      - kafka-controller-1
      - kafka-controller-2

  # kafka-exporter:
  #   image: danielqsj/kafka-exporter:v1.8.0
  #   ports:
  #     - "9308:9308"
  #   command: ["--kafka.server=kafka-broker-0:9092", "--kafka.server=kafka-broker-1:9092", "--kafka.server=kafka-broker-2:9092"]
  #   restart: on-failure
  #   depends_on:
  #     # - logstash
  #     - kafka-broker-0
  #     - kafka-broker-1
  #     - kafka-broker-2

  # data-generation:
  #   image: dip.ad.atsonline.de:5000/cloud_evaluation/data-generation:latest
  #   restart: always
  #   ports:
  #     - "5247:5247"

  # producer-ats:
  #   image: dip.ad.atsonline.de:5000/cloud_evaluation/producer:ssl
  #   ports:
  #     - "5131:5131"
  #   restart: always
  #   environment:
  #     - ASPNETCORE_ENVIRONMENT=Development
  #     - Kafka__Brokers=kafka-broker-0:9094,kafka-broker-1:9095,kafka-broker-2:9096
  #     - Kafka__Create_Topics__0=ats1-iot-data
  #     - Kafka__Create_Topics__1=ats2-iot-data
  #     - Kafka__Create_Topics__2=ats3-iot-data
  #     - Companies__ATS1_API=http://data-generation:5247/data/ats1
  #     - Companies__ATS1_Topic=ats1-iot-data
  #     - Companies__ATS1_Delay=1000
  #     - Companies__ATS2_API=http://data-generation:5247/data/ats2
  #     - Companies__ATS2_Topic=ats2-iot-data
  #     - Companies__ATS2_Delay=5000
  #     - Companies__ATS3_API=http://data-generation:5247/data/ats3
  #     - Companies__ATS3_Topic=ats3-iot-data
  #     - Companies__ATS3_Delay=10000
  #     - Certificate__CAPath=${CERTIFICATE_CA_PATH}
  #     - Certificate__Path=${CERTIFICATE_PATH}
  #     - Certificate__Password=${CERTIFICATE_PASSWORD}
  #   # logging:
  #   #   driver: gelf
  #   #   options:
  #   #     gelf-address: ${GELF_ADDRESS}
  #   #     tag: "producer-ats"
  #   volumes:
  #     - ./Producers/Certificates:/app/Certificates
  #   # deploy:
  #   #   resources:
  #   #     limits:
  #   #       cpus: "1.0"
  #   #       memory: "512M"
  #   depends_on:
  #     # logstash:
  #     #   condition: service_started
  #     kafka-broker-0:
  #       condition: service_started
  #     kafka-broker-1:
  #       condition: service_started
  #     kafka-broker-2:
  #       condition: service_started
  #     data-generation:
  #       condition: service_started

  # consumer-ats:
  #   image: dip.ad.atsonline.de:5000/cloud_evaluation/consumer:ssl
  #   ports:
  #     - "5121:5121"
  #   restart: always
  #   environment:
  #     - ASPNETCORE_ENVIRONMENT=Development
  #     - Kafka__Brokers=kafka-broker-0:9094,kafka-broker-1:9095,kafka-broker-2:9096
  #     - Kafka__Consumer_Group=ats-group
  #     - Kafka__Create_Topics__0=ats1-iot-data
  #     - Kafka__Create_Topics__1=ats2-iot-data
  #     - Kafka__Create_Topics__2=ats3-iot-data
  #     - Kafka__Subscribe_Topics__0=ats1-iot-data
  #     - Kafka__Subscribe_Topics__1=ats2-iot-data
  #     - Kafka__Subscribe_Topics__2=ats3-iot-data
  #     - MongoDB__ConnectionStrings="mongodb://mongo0:27017,mongo1:27018,mongo2:27019/?replicaSet=rs0"
  #     - Certificate__CAPath=${CERTIFICATE_CA_PATH}
  #     - Certificate__Path=${CERTIFICATE_PATH}
  #     - Certificate__Password=${CERTIFICATE_PASSWORD}
  #   # logging:
  #   #   driver: gelf
  #   #   options:
  #   #     gelf-address: ${GELF_ADDRESS}
  #   #     tag: "consumer-ats"
  #   volumes:
  #     - ./Consumers/Certificates:/app/Certificates
  #   # deploy:
  #   #   resources:
  #   #     limits:
  #   #       cpus: "1.0"
  #   #       memory: "512M"
  #   depends_on:
  #     # logstash:
  #     #   condition: service_started
  #     kafka-broker-0:
  #       condition: service_started
  #     kafka-broker-1:
  #       condition: service_started
  #     kafka-broker-2:
  #       condition: service_started
  #     mongo0:
  #       condition: service_started
  #     mongo1:
  #       condition: service_started
  #     mongo2:
  #       condition: service_started

  producer-vds:
    image: dip.ad.atsonline.de:5000/cloud_evaluation/tcp-server
    container_name: producer-vds
    restart: always
    ports:
      - "50000:50000"
      - "50001:50001"
    environment:
      - Port=50000
      - MetricsPort=50001
      - Kafka__Use=true
      - Kafka__Brokers=kafka-broker-0:9092,kafka-broker-1:9092,kafka-broker-2:9092
      - Kafka__Topic=tcp-data
    # logging:
    #   driver: gelf
    #   options:
    #     gelf-address: ${GELF_ADDRESS}
    #     tag: "producer-vds"
    # depends_on:
    #   - logstash

  consumer-vds:
    image: dip.ad.atsonline.de:5000/cloud_evaluation/consumer:tcp-v2
    container_name: consumer-vds
    ports:
      - "5121:5121"
    restart: always
    environment:
      - ASPNETCORE_ENVIRONMENT=Development
      - Kafka__Brokers=kafka-broker-0:9092,kafka-broker-1:9092,kafka-broker-2:9092
      - Kafka__Consumer_Group=tcp-group
      - Kafka__Create_Topics__0=tcp-data
      - Kafka__Create_Topics__1=tcp-data-ack
      - Kafka__Subscribe_Topics__0=tcp-data
      - MongoDB__ConnectionStrings="mongodb://mongo0:27017,mongo1:27018,mongo2:27019/?replicaSet=rs0"
     # logging:
    #   driver: gelf
    #   options:
    #     gelf-address: ${GELF_ADDRESS}
    #     tag: "consumer-vds"
    depends_on:
      # - logstash
      - kafka-broker-0
      - kafka-broker-1
      - kafka-broker-2
      - mongo0
      - mongo1
      - mongo2

  consumer-vds-ack:
    image: dip.ad.atsonline.de:5000/cloud_evaluation/consumer:ack
    hostname: consumer-vds
    ports:
      - "5131:5131"
    environment:
      - ASPNETCORE_ENVIRONMENT=Development
      - Kafka__Brokers=kafka-broker-0:9092,kafka-broker-1:9092,kafka-broker-2:9092
      - Kafka__Consumer_Group=tcp-group
      - Kafka__Create_Topics__0=tcp-data-ack
      - Kafka__Subscribe_Topics__0=tcp-data-ack
      # - ConnectionStrings__PostgresConnection="Host=localhost;Port=5432;Database=KafkaData;Username=postgres;Password=postgres"

  webapi:
    image: dip.ad.atsonline.de:5000/cloud_evaluation/webapi:tcp-v2
    hostname: webapi
    ports:
      - "5046:5046"
    environment:
      - ConnectionStrings__PostgresConnection=Host=host.docker.internal;Port=5432;Database=KafkaData;Username=postgres;Password=postgres
      - ConnectionStrings__Topic=tcp-data-ack

  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: KafkaData
    volumes:
      - postgres-data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  adminer:
    image: adminer
    restart: always
    ports:
      - 8084:8080

  # prometheus:
  #   image: prom/prometheus
  #   restart: on-failure
  #   ports:
  #     - "9090:9090"
  #   volumes:
  #     - ./prometheus.yml:/etc/prometheus/prometheus.yml
  #   depends_on:
  #     - kafka-exporter
  #     # - consumer-vds
  #     - producer-ats
  #     - consumer-ats

  # grafana:
  #   image: grafana/grafana:11.5.2
  #   restart: on-failure
  #   ports:
  #     - "3000:3000"
  #   depends_on:
  #     - prometheus
  #   volumes:
  #     - grafana_data:/var/lib/grafana

  # akhq:
  #   image: tchiotludo/akhq
  #   ports:
  #     - "8080:8080"
  #   environment:
  #     AKHQ_CONFIGURATION: |
  #       akhq:
  #         connections:
  #           kafka:
  #             properties:
  #               bootstrap.servers: "kafka-broker-0:9092,kafka-broker-1:9092,kafka-broker-2:9092"

  mongo0:
    image: mongo:8.0.4
    restart: always
    command: ["--replSet", "rs0",  "--bind_ip_all", "--port", "27017"]
    healthcheck:
      test: echo "try { rs.status() } catch (err) { rs.initiate({_id:'rs0',members:[{_id:0,host:'mongo0:27017',priority:2},{_id:1,host:'mongo1:27018',priority:1},{_id:2,host:'mongo2:27019',priority:0.5}]}) }" | mongosh --port 27017 --quiet
      interval: 5s
      timeout: 30s
      start_period: 0s
      start_interval: 1s
      retries: 30
    ports:
      - 27017:27017
    volumes:
      - mongo0_data:/data/db
    # logging:
    #   driver: gelf
    #   options:
    #     gelf-address: ${GELF_ADDRESS}
    #     tag: "mongo0"
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "0.5"
    #       memory: "512M"
    # depends_on:
    #   - logstash

  mongo1:
    image: mongo:8.0.4
    restart: always
    command: ["--replSet", "rs0",  "--bind_ip_all", "--port", "27018"]
    ports:
      - 27018:27018
    volumes:
      - mongo1_data:/data/db
    # logging:
    #   driver: gelf
    #   options:
    #     gelf-address: ${GELF_ADDRESS}
    #     tag: "mongo1"
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "0.5"
    #       memory: "512M"
    # depends_on:
    #   - logstash

  mongo2:
    image: mongo:8.0.4
    restart: always
    command: ["--replSet", "rs0", "--bind_ip_all", "--port", "27019"]
    ports:
      - 27019:27019
    volumes:
      - mongo2_data:/data/db
    # logging:
    #   driver: gelf
    #   options:
    #     gelf-address: ${GELF_ADDRESS}
    #     tag: "mongo2"
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "0.5"
    #       memory: "512M"
    # depends_on:
    #   - logstash

  mongo-express:
    image: mongo-express:latest
    ports:
      - 8081:8081
    environment: 
      - ME_CONFIG_MONGODB_URL=mongodb://mongo0:27017,mongo1:27018,mongo2:27019
    depends_on:
      - mongo0
      - mongo1
      - mongo2
      
  # mongodb-exporter-bitnami:
  #   image: bitnami/mongodb-exporter:latest
  #   command: --collect-all --discovering-mode --no-mongodb.direct-connect --mongodb.uri="mongodb://mongo0:27017/admin?ssl=false" --web.listen-address=":9216" --web.telemetry-path="/metrics" --mongodb.indexstats-colls=KafaData.KafkaEntry --mongodb.collstats-colls=KafaData.KafkaEntry
  #   # command: [" /opt/bitnami/mongodb-exporter/bin/mongodb_exporter --discovering-mode --mongodb.indexstats-colls=KafaData.KafkaEntry --mongodb.collstats-colls=KafaData.KafkaEntry --web.listen-address=\":9216\" --web.telemetry-path=\"/metrics\" --no-mongodb.direct-connect --mongodb.uri=\"mongodb://mongo0:27017/admin?ssl=false\""]
  #   # args: ["-c", '/opt/bitnami/mongodb-exporter/bin/mongodb_exporter --discovering-mode --web.listen-address=":9216" --web.telemetry-path="/metrics" --mongodb.direct-connect=false --mongodb.uri="mongodb://mongo0:27017/admin?ssl=false"']
    
  #   environment:
  #     MONGODB_URI: mongodb://mongo0:27017/admin?ssl=false
  #     # MONGODB_INDEXSTATS-COLLS: KafaData.KafkaEntry
  #     # MONGODB_COLLSTATS-COLLS: KafaData.KafkaEntry
  #   ports:
  #     - 9216:9216
  #   depends_on:
  #     - mongo0
  #     - mongo1
  #     - mongo2

  # mongodb-exporter-percona:
  #   image: percona/mongodb_exporter:0.43.1
  #   command: --mongodb.uri="mongodb://mongo0:27017/admin" --collect-all --discovering-mode
  #   ports:
  #     - "9216:9216"
  #   depends_on:
  #     - mongo0
  #     - mongo1
  #     - mongo2

  # elasticsearch:
  #     image: docker.elastic.co/elasticsearch/elasticsearch:8.15.1
  #     container_name: elasticsearch
  #     restart: always
  #     environment:
  #       - discovery.type=single-node
  #       - xpack.security.enabled=false
  #       - bootstrap.memory_lock=true
  #       - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
  #     # deploy:
  #     #   resources:
  #     #     limits:
  #     #       cpus: "1.0"
  #     #       memory: "1G"
  #     volumes:
  #       - elastic_data:/usr/share/elasticsearch/data
  #     ports:
  #       - "9200:9200"
  #     logging:
  #       driver: gelf
  #       options:
  #         gelf-address: ${GELF_ADDRESS}
  #         tag: "elasticsearch"
  #     depends_on:
  #       - logstash

  # logstash:
  #   image: docker.elastic.co/logstash/logstash:8.15.1
  #   container_name: logstash
  #   restart: always
  #   volumes:
  #     - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
  #   ports:
  #     - "5044:5044"

  # kibana:
  #   image: docker.elastic.co/kibana/kibana:8.15.1
  #   container_name: kibana
  #   restart: on-failure
  #   environment:
  #     - ELASTICSEARCH_URL=${ELASTICSEARCH_URL}
  #     - ELASTICSEARCH_USERNAME=${ELASTICSEARCH_USERNAME}
  #     - ELASTICSEARCH_PASSWORD=${ELASTICSEARCH_PASSWORD}
  #   ports:
  #     - "8082:5601"
  #   depends_on:
  #     - elasticsearch
  #   volumes:
  #     - kibana_data:/usr/share/kibana/data

volumes:
  kafka_controller_data_0:
    driver: local
  kafka_controller_data_1:
    driver: local
  kafka_controller_data_2:
    driver: local
  kafka_data_0:
    driver: local
  kafka_data_1:
    driver: local
  kafka_data_2:
    driver: local
  mongo0_data:
    driver: local
  mongo1_data:
    driver: local
  mongo2_data:
    driver: local
  postgres-data:
    driver: local
  # grafana_data:
  #   driver: local
  # elastic_data:
  #   driver: local
  # kibana_data:
  #   driver: local